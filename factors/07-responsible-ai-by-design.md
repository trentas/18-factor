# Factor 7: Responsible AI by Design

> Build safety, fairness, and accountability into the architecture from the start — not as an afterthought or a compliance checkbox.

## Motivation

Traditional application security focuses on protecting the system from external threats: injection attacks, unauthorized access, data breaches. AI systems introduce a new class of risks that come from within the system itself. A model can generate harmful content, leak private information from its training data, produce biased outputs that discriminate against protected groups, or be manipulated through adversarial prompts.

These aren't bugs in the traditional sense — they're emergent properties of systems that learn from data and generate novel outputs. You can't fix them with a patch. They require architectural patterns: guardrails, monitoring, human oversight, and continuous evaluation. Responsible AI isn't a feature — it's a cross-cutting architectural concern, like security or observability.

## What This Replaces

**New — no direct predecessor.** The original 12/15-factor methodology predates the era where applications could generate harmful, biased, or misleading content as a normal part of their operation.

The closest analogue is security best practices, but responsible AI encompasses fairness, transparency, privacy, and safety concerns that go beyond traditional security.

## How AI Changes This

This factor *is* the AI change. It exists because AI systems can:

- **Generate harmful content**: Toxic, violent, sexual, or otherwise harmful outputs.
- **Leak private information**: Models can memorize and reproduce training data, including PII.
- **Produce biased outputs**: Systematic discrimination based on protected characteristics.
- **Be manipulated**: Prompt injection, jailbreaking, and adversarial inputs can subvert intended behavior.
- **Hallucinate**: Confidently state false information, including fabricated citations and non-existent URLs.
- **Act beyond intended scope**: Agents may take actions that exceed their intended authority.

## In Practice

### Safety Layers Architecture
Implement defense in depth — multiple layers, each catching different issues:

```
┌──────────────────────────────────────────────┐
│              INPUT GUARDRAILS                │
│  Prompt injection detection                  │
│  Input validation and sanitization           │
│  PII detection and redaction                 │
│  Content policy pre-screening                │
├──────────────────────────────────────────────┤
│              MODEL LAYER                     │
│  System prompt with safety instructions      │
│  Constrained output schemas                  │
│  Temperature and sampling controls           │
├──────────────────────────────────────────────┤
│              OUTPUT GUARDRAILS               │
│  Content safety classification               │
│  PII detection in outputs                    │
│  Hallucination detection                     │
│  Fact-checking against sources               │
├──────────────────────────────────────────────┤
│              MONITORING LAYER                │
│  Safety metric tracking                      │
│  Bias detection and alerting                 │
│  Human review queues                         │
│  Incident response triggers                  │
└──────────────────────────────────────────────┘
```

### Prompt Injection Defense
Protect against attempts to override system instructions:

```python
# Multi-layer prompt injection defense
class InputGuardrail:
    def check(self, user_input: str) -> GuardrailResult:
        results = []

        # Layer 1: Pattern matching for known injection patterns
        results.append(self.pattern_detector.scan(user_input))

        # Layer 2: ML-based injection classifier
        results.append(self.injection_classifier.classify(user_input))

        # Layer 3: Input/output boundary enforcement
        results.append(self.boundary_enforcer.check(user_input))

        return GuardrailResult.aggregate(results)
```

### PII Handling
Detect and handle personally identifiable information at system boundaries:

```yaml
pii_policy:
  input:
    detection_enabled: true
    action: redact_and_log           # redact_and_log | block | allow_and_flag
    entity_types:
      - email
      - phone_number
      - ssn
      - credit_card
      - address
      - date_of_birth

  output:
    detection_enabled: true
    action: block                     # Stricter on output — never leak PII
    fallback_response: "I can't include personal information in my response."

  storage:
    redacted_in_logs: true
    redacted_in_traces: true
    retention_days: 30
```

### Bias Monitoring
Continuously monitor for systematic biases in AI outputs:

```python
# Bias monitoring across demographic dimensions
class BiasMonitor:
    def evaluate(self, requests, responses):
        # Segment by demographic indicators
        segments = self.segment_by_demographics(requests)

        for dimension in ["gender", "ethnicity", "age_group", "language"]:
            metrics = {}
            for segment_value, segment_data in segments[dimension].items():
                metrics[segment_value] = {
                    "quality_score": self.evaluate_quality(segment_data),
                    "refusal_rate": self.measure_refusal_rate(segment_data),
                    "response_length": self.measure_response_length(segment_data),
                    "sentiment": self.measure_sentiment(segment_data),
                }

            # Alert on significant disparities
            if self.detect_disparity(metrics):
                self.alert(dimension, metrics)
```

### Human-in-the-Loop Gates
Define clear criteria for *when* human oversight is required. This factor owns the trigger conditions; Factor 8 defines *who* can approve, and Factor 17 defines *how* approval is executed at runtime.

```yaml
human_review_triggers:
  # Content-based triggers
  - condition: safety_score < 0.7
    action: block_and_queue_for_review
    approval_timeout_seconds: 1800

  # Action-based triggers
  - condition: agent_action in [delete, publish, send_email, financial_transaction]
    action: require_approval_before_execution
    approval_timeout_seconds: 300

  # Confidence-based triggers
  - condition: model_confidence < 0.5
    action: flag_for_review_before_serving
    approval_timeout_seconds: 900

  # Volume-based triggers
  - condition: user_requests_per_hour > 100
    action: sample_and_review
    sample_rate: 0.10
```

### Transparency and Explainability
Make AI decisions auditable:

- **Disclosure**: Clearly indicate when content is AI-generated.
- **Attribution**: When RAG is used, cite the source documents.
- **Reasoning traces**: Log the chain-of-thought or reasoning steps for important decisions.
- **Confidence indicators**: Surface confidence scores to users and downstream systems.
- **Appeal mechanisms**: Provide paths for users to contest AI decisions.

### Data Governance for AI
AI systems consume, generate, and store data with unique governance challenges:

```yaml
data_governance:
  training_data:
    provenance: documented        # track origin of all training/fine-tuning data
    licensing: verified           # confirm data usage rights before training
    pii_handling: anonymized      # no PII in training datasets without consent

  conversation_logs:
    retention_days: 90            # define retention period per regulation
    right_to_erasure: supported   # users can request deletion of their data
    use_for_training: opt_in      # never use production logs for training without consent

  rag_sources:
    provenance: tracked           # document source, update date, license
    refresh_cadence: weekly       # stale data degrades quality
    access_controls: enforced     # respect source document permissions

  synthetic_data:
    labeled_as_synthetic: true    # never mix synthetic with real without labeling
    generation_method: documented # reproducibility
```

### Regulatory Compliance (GDPR / LGPD)
AI applications that process personal data must comply with data protection regulations. These requirements are architectural — they must be built into the system, not bolted on after launch.

- **Legal basis for processing**: Document the legal basis (consent, legitimate interest, contractual necessity) for every AI feature that processes personal data. Consent must be granular — "use my data to improve the product" is not valid consent for AI training.
- **Data minimization**: Collect and process only the data strictly necessary for the AI task. If a summarization feature doesn't need the user's name, strip it before sending to the model.
- **Right to erasure**: Users can request deletion of their personal data, including data used in conversation logs, fine-tuning datasets, and vector stores. The system must be able to locate and delete all instances.
- **Data Processing Impact Assessment (DPIA)**: High-risk AI features — those that profile users, make automated decisions, or process sensitive data — require a DPIA before launch.
- **Cross-border data transfers**: When AI models or APIs are hosted in a different jurisdiction than the user's data, ensure adequate transfer mechanisms (SCCs, adequacy decisions) are in place.
- **Data processor agreements**: Third-party AI providers (model APIs, embedding services) are data processors. Ensure DPAs are signed and their data handling policies are compatible with your obligations.

### Non-Production Data Anonymization
Non-production environments (dev, staging, QA) must never contain real personal data. This is one of the most common — and most preventable — regulatory violations.

```yaml
non_production_data:
  policy: anonymize_before_copy     # never copy production data without anonymization

  anonymization:
    strategy: pseudonymization       # pseudonymization | synthetic_generation | k-anonymity
    fields:
      - type: name
        method: fake_name            # generate realistic fake names
      - type: email
        method: hash_and_domain      # user@company.com → a3f8c@test.example.com
      - type: phone
        method: randomize            # preserve format, randomize digits
      - type: address
        method: generalize           # keep city/state, remove street
      - type: free_text
        method: ner_redact           # NER-based PII redaction in unstructured text
      - type: date_of_birth
        method: shift                # shift by random offset, preserve age distribution

  validation:
    scan_after_copy: true            # run PII scanner on anonymized dataset
    block_on_pii_detected: true      # fail pipeline if PII leaks through
    audit_log: true                  # log who copied what and when

  refresh_cadence: monthly           # re-anonymize from production monthly for freshness
```

This is especially critical when AI agents autonomously create branches and spin up ephemeral environments (Factor 11) — automated pipelines must use anonymized datasets by default, never production data.

### Red Teaming and Adversarial Testing
Defensive guardrails are necessary but not sufficient. You must actively test them. Red teaming is the practice of systematically probing an AI system for vulnerabilities — prompt injection bypasses, harmful output generation, PII extraction, bias exploitation, and jailbreaks.

```yaml
red_team_program:
  cadence: quarterly                  # full red team exercise
  continuous: true                    # automated adversarial evals run in CI

  attack_categories:
    - prompt_injection:               # attempts to override system instructions
        methods: [direct_injection, indirect_injection, payload_splitting]
    - jailbreak:                      # attempts to bypass safety boundaries
        methods: [roleplay, encoding_tricks, multi_turn_escalation]
    - data_extraction:                # attempts to extract training data or PII
        methods: [memorization_probing, context_extraction, system_prompt_leak]
    - bias_exploitation:              # attempts to trigger biased outputs
        methods: [demographic_probing, stereotype_elicitation]
    - output_manipulation:            # attempts to generate harmful content
        methods: [indirect_harmful, dual_use, gradual_escalation]

  adversarial_eval_dataset:
    source: evals/adversarial.jsonl   # versioned alongside code (Factor 1)
    min_samples: 200
    refresh: after_each_exercise      # add new attack vectors discovered

  response:
    vulnerability_found:
      - add_to_adversarial_dataset
      - create_guardrail_or_fix
      - re_run_eval_suite
      - update_incident_playbook
```

Red teaming is not a one-time audit — it's a continuous practice. Every model upgrade, prompt change, or new feature should trigger adversarial evaluation. Automated adversarial eval suites (Factor 6) complement but do not replace human-led exercises.

### Incident Response for AI
AI incidents require specific response procedures:

```yaml
ai_incident_playbook:
  severity_1_harmful_output:
    - immediately: disable_affected_feature
    - within_1h: identify_root_cause_and_affected_scope
    - within_4h: deploy_fix_or_guardrail
    - within_24h: conduct_retrospective
    - within_1w: add_to_eval_suite

  severity_2_bias_detected:
    - immediately: flag_for_investigation
    - within_24h: quantify_impact_and_scope
    - within_1w: implement_mitigation
    - ongoing: enhanced_monitoring
```

## Compliance Checklist

- [ ] Input guardrails detect and handle prompt injection attempts
- [ ] PII is detected at input and output boundaries with configurable handling policies
- [ ] Content safety classifiers screen AI outputs before serving to users
- [ ] Bias monitoring evaluates output quality, refusal rates, and sentiment across demographic segments on a defined cadence
- [ ] Human-in-the-loop gates are defined for high-risk actions and low-confidence outputs
- [ ] AI-generated content is clearly disclosed to users
- [ ] Non-production environments use anonymized data with automated PII scanning to prevent leaks
- [ ] Regulatory compliance (GDPR/LGPD) is addressed: legal basis, data minimization, right to erasure, and DPIA for high-risk AI features
- [ ] An AI incident response playbook exists and is practiced
- [ ] Red teaming exercises run on a defined cadence with adversarial eval datasets maintained in CI
