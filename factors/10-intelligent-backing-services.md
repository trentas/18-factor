# Factor 10: Intelligent Backing Services

> Treat all backing services — databases, queues, LLM providers, vector stores, and embedding services — as attached resources, swappable via configuration.

## Motivation

The original factor established that backing services (databases, message queues, SMTP servers, caching systems) should be treated as attached resources, accessible via URLs or connection strings in configuration. A database is just a resource — whether it's local or managed, you swap it by changing a config value.

AI applications introduce a new category of backing services that are central to the application's intelligence: LLM inference providers, vector databases, embedding services, model registries, and evaluation platforms. These must be treated with the same discipline as traditional backing services — attached via configuration, swappable between providers, and resilient to outages.

## What This Replaces

**Original Factor #4 / Beyond 15 #8: Backing Services** — "Treat backing services as attached resources."

This update retains the core principle and extends the catalog of backing services to include:

- LLM inference providers (OpenAI, Anthropic, Google, self-hosted)
- Vector databases (Pinecone, Weaviate, Qdrant, pgvector)
- Embedding services (OpenAI, Cohere, self-hosted models)
- Model registries (MLflow, W&B, HuggingFace Hub)
- Reranking services
- Content safety and moderation APIs
- Evaluation and monitoring platforms

## How AI Changes This

### AI-Assisted Development
- AI coding assistants themselves are backing services — they depend on LLM provider APIs. When the provider has an outage, developer productivity degrades. Having fallback options (local models, alternative providers) maintains developer velocity.

### AI-Native Applications
- **LLM providers as resources**: An LLM provider is a backing service. Switching from OpenAI to Anthropic should be a configuration change, not a rewrite. This requires an abstraction layer over provider-specific SDKs.
- **Vector databases as resources**: A vector database stores the embeddings that power semantic search and RAG. Whether you use Pinecone, Weaviate, or pgvector, the application code should work through an abstraction.
- **Embedding services as resources**: Embeddings are generated by models. The embedding provider should be swappable — but note that changing embedding models requires re-embedding all data (this is a major operational consideration).
- **Multi-provider resilience**: Unlike traditional databases where you have one primary, AI applications benefit from multi-provider strategies — primary and fallback LLM providers, multiple embedding sources, etc.

## In Practice

### Resource Configuration

```yaml
# backing-services.yaml
services:
  llm:
    primary:
      provider: anthropic
      endpoint: https://api.anthropic.com
      model: claude-sonnet-4-5-20250929
      credentials: vault://secrets/anthropic-api-key
      timeout_ms: 30000
      max_retries: 3
    fallback:
      provider: openai
      endpoint: https://api.openai.com
      model: gpt-4o
      credentials: vault://secrets/openai-api-key
      timeout_ms: 30000

  vector_db:
    provider: pinecone
    endpoint: https://my-index.pinecone.io
    credentials: vault://secrets/pinecone-api-key
    index_name: production-embeddings
    dimension: 1536
    metric: cosine

  embedding:
    provider: openai
    model: text-embedding-3-small
    endpoint: https://api.openai.com
    credentials: vault://secrets/openai-api-key
    dimension: 1536
    batch_size: 100

  reranker:
    provider: cohere
    model: rerank-english-v3.0
    endpoint: https://api.cohere.ai
    credentials: vault://secrets/cohere-api-key

  cache:
    provider: redis
    endpoint: redis://cache-cluster:6379
    purpose: semantic_cache
```

### Provider Abstraction Layer
Abstract backing service interfaces to enable swapping:

```python
# Abstract interface — implementation swapped via config
class LLMProvider(Protocol):
    async def complete(self, messages: list[Message], **kwargs) -> Completion: ...
    async def stream(self, messages: list[Message], **kwargs) -> AsyncIterator[Chunk]: ...

class VectorStore(Protocol):
    async def upsert(self, vectors: list[Vector]) -> None: ...
    async def query(self, vector: list[float], top_k: int) -> list[SearchResult]: ...
    async def delete(self, ids: list[str]) -> None: ...

class EmbeddingService(Protocol):
    async def embed(self, texts: list[str]) -> list[list[float]]: ...
    @property
    def dimension(self) -> int: ...
```

### Failover and Resilience

```python
class ResilientLLMProvider:
    def __init__(self, primary: LLMProvider, fallback: LLMProvider):
        self.primary = primary
        self.fallback = fallback
        self.circuit_breaker = CircuitBreaker(
            failure_threshold=5,
            recovery_timeout=60,
        )

    async def complete(self, messages, **kwargs):
        try:
            if self.circuit_breaker.is_open:
                raise CircuitOpenError()
            result = await self.primary.complete(messages, **kwargs)
            self.circuit_breaker.record_success()
            return result
        except (ProviderError, CircuitOpenError):
            self.circuit_breaker.record_failure()
            # Automatic fallback — same interface, different provider
            return await self.fallback.complete(messages, **kwargs)
```

### Attachment and Detachment
Like traditional backing services, AI backing services should be attachable and detachable:

- **Attach a new vector DB**: Point the configuration at a new endpoint, run re-indexing if needed.
- **Swap LLM providers**: Change the provider configuration, validate with evaluations (Factor 6).
- **Replace embedding service**: Change the provider — but this requires re-embedding all data, so plan for migration.
- **Scale independently**: LLM providers, vector databases, and embedding services scale on their own axes. Don't couple their scaling to the application.

### Operational Considerations
AI backing services have unique operational characteristics:

- **Cost variability**: LLM providers charge per token, vector DBs charge per query and storage, embedding services charge per text. Monitor costs per service.
- **Latency profiles**: LLM inference (100ms–30s), embedding (10–100ms), vector search (5–50ms). Understand the latency budget for each service in your request chain.
- **Rate limits**: Most AI services have rate limits (requests per minute, tokens per minute). Implement client-side rate limiting and queuing.
- **Data residency**: Some AI providers process data in specific regions. Ensure compliance with data residency requirements.
- **Vendor lock-in**: The more provider-specific features you use (fine-tuning, proprietary APIs), the harder it is to swap. Keep a clean abstraction layer.

## Compliance Checklist

- [ ] All backing services (including AI services) are attached via configuration, not hardcoded
- [ ] LLM providers are abstracted behind a common interface, enabling provider swapping
- [ ] Vector database access goes through an abstraction layer
- [ ] Embedding service changes are planned as data migration operations
- [ ] Failover strategies exist for critical AI backing services (LLM providers, vector DBs)
- [ ] Circuit breakers protect against cascading failures from AI service outages
- [ ] Rate limiting is implemented client-side for all AI backing services
- [ ] Credentials for AI services are managed through secrets management (Factor 4)
- [ ] Cost, latency, and availability are monitored per backing service
- [ ] Backing service dependencies are documented with their operational characteristics
